{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time as t\n",
    "\n",
    "from tensorflow import convert_to_tensor, float32, tensordot\n",
    "from tensorflow import abs as tfabs\n",
    "from tensorflow.io import serialize_tensor, write_file\n",
    "from tensorflow.math import log\n",
    "from tensorflow.signal import linear_to_mel_weight_matrix,mfccs_from_log_mel_spectrograms, stft\n",
    "dataset_dir = 'data/mini_speech_commands_datasets'\n",
    "\n",
    "mods_names = ['mlp','cnn','dscnn']\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = keras.metrics.SparseCategoricalAccuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 1.8282 - sparse_categorical_accuracy: 0.3791WARNING:tensorflow:From /home/ema/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/ema/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./callback_test_chkp/mlp_chkp_best/assets\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 1.8172 - sparse_categorical_accuracy: 0.3833 - val_loss: 1.5787 - val_sparse_categorical_accuracy: 0.4725\n",
      "Epoch 2/20\n",
      "196/200 [============================>.] - ETA: 0s - loss: 1.4065 - sparse_categorical_accuracy: 0.5271INFO:tensorflow:Assets written to: ./callback_test_chkp/mlp_chkp_best/assets\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.4043 - sparse_categorical_accuracy: 0.5277 - val_loss: 1.3781 - val_sparse_categorical_accuracy: 0.5325\n",
      "Epoch 3/20\n",
      "194/200 [============================>.] - ETA: 0s - loss: 1.1194 - sparse_categorical_accuracy: 0.6089INFO:tensorflow:Assets written to: ./callback_test_chkp/mlp_chkp_best/assets\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1222 - sparse_categorical_accuracy: 0.6089 - val_loss: 1.2879 - val_sparse_categorical_accuracy: 0.5700\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.9534 - sparse_categorical_accuracy: 0.6639 - val_loss: 1.3827 - val_sparse_categorical_accuracy: 0.5775\n",
      "Epoch 5/20\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 0.8201 - sparse_categorical_accuracy: 0.7101INFO:tensorflow:Assets written to: ./callback_test_chkp/mlp_chkp_best/assets\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.8246 - sparse_categorical_accuracy: 0.7086 - val_loss: 1.2208 - val_sparse_categorical_accuracy: 0.6500\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.7243 - sparse_categorical_accuracy: 0.7434 - val_loss: 1.2801 - val_sparse_categorical_accuracy: 0.6187\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6116 - sparse_categorical_accuracy: 0.7809 - val_loss: 1.3131 - val_sparse_categorical_accuracy: 0.6338\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5563 - sparse_categorical_accuracy: 0.8008 - val_loss: 1.4182 - val_sparse_categorical_accuracy: 0.6363\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5359 - sparse_categorical_accuracy: 0.8111 - val_loss: 1.4019 - val_sparse_categorical_accuracy: 0.6388\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4366 - sparse_categorical_accuracy: 0.8461 - val_loss: 1.4604 - val_sparse_categorical_accuracy: 0.6450\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4718 - sparse_categorical_accuracy: 0.8378 - val_loss: 1.5351 - val_sparse_categorical_accuracy: 0.6538\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3616 - sparse_categorical_accuracy: 0.8719 - val_loss: 1.6133 - val_sparse_categorical_accuracy: 0.6562\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3826 - sparse_categorical_accuracy: 0.8712 - val_loss: 1.8816 - val_sparse_categorical_accuracy: 0.6062\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3103 - sparse_categorical_accuracy: 0.8947 - val_loss: 1.7754 - val_sparse_categorical_accuracy: 0.6612\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3295 - sparse_categorical_accuracy: 0.8917 - val_loss: 1.6830 - val_sparse_categorical_accuracy: 0.6550\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2601 - sparse_categorical_accuracy: 0.9173 - val_loss: 1.8034 - val_sparse_categorical_accuracy: 0.6500\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2252 - sparse_categorical_accuracy: 0.9216 - val_loss: 2.2052 - val_sparse_categorical_accuracy: 0.6587\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2266 - sparse_categorical_accuracy: 0.9217 - val_loss: 2.1844 - val_sparse_categorical_accuracy: 0.6687\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2017 - sparse_categorical_accuracy: 0.9386 - val_loss: 2.0969 - val_sparse_categorical_accuracy: 0.6550\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2110 - sparse_categorical_accuracy: 0.9345 - val_loss: 2.2192 - val_sparse_categorical_accuracy: 0.6538\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 2056      \n",
      "=================================================================\n",
      "Total params: 396,040\n",
      "Trainable params: 396,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "25/25 - 0s - loss: 2.1615 - sparse_categorical_accuracy: 0.6250\n",
      "\n",
      "acc: 0.625, size: 112517 Inference Latency 0.07861328125ms\n",
      "\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8646 - sparse_categorical_accuracy: 0.3483INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 1.8646 - sparse_categorical_accuracy: 0.3483 - val_loss: 1.6378 - val_sparse_categorical_accuracy: 0.4263\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4916 - sparse_categorical_accuracy: 0.4827INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 1.4916 - sparse_categorical_accuracy: 0.4827 - val_loss: 1.4126 - val_sparse_categorical_accuracy: 0.4963\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2910 - sparse_categorical_accuracy: 0.5528INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 19s 93ms/step - loss: 1.2910 - sparse_categorical_accuracy: 0.5528 - val_loss: 1.2996 - val_sparse_categorical_accuracy: 0.5138\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1553 - sparse_categorical_accuracy: 0.6084INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 23s 116ms/step - loss: 1.1553 - sparse_categorical_accuracy: 0.6084 - val_loss: 1.2264 - val_sparse_categorical_accuracy: 0.5475\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0706 - sparse_categorical_accuracy: 0.6373INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 19s 94ms/step - loss: 1.0706 - sparse_categorical_accuracy: 0.6373 - val_loss: 1.2188 - val_sparse_categorical_accuracy: 0.5487\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.9932 - sparse_categorical_accuracy: 0.6587INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 23s 114ms/step - loss: 0.9932 - sparse_categorical_accuracy: 0.6587 - val_loss: 1.0673 - val_sparse_categorical_accuracy: 0.6212\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 17s 87ms/step - loss: 0.9327 - sparse_categorical_accuracy: 0.6800 - val_loss: 1.0979 - val_sparse_categorical_accuracy: 0.5763\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8818 - sparse_categorical_accuracy: 0.6994INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 0.8818 - sparse_categorical_accuracy: 0.6994 - val_loss: 1.0147 - val_sparse_categorical_accuracy: 0.6388\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8195 - sparse_categorical_accuracy: 0.7228INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 0.8195 - sparse_categorical_accuracy: 0.7228 - val_loss: 0.9308 - val_sparse_categorical_accuracy: 0.6950\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.7915 - sparse_categorical_accuracy: 0.7350 - val_loss: 1.0358 - val_sparse_categorical_accuracy: 0.6338\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 0.7570 - sparse_categorical_accuracy: 0.7461 - val_loss: 0.9620 - val_sparse_categorical_accuracy: 0.6737\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7045 - sparse_categorical_accuracy: 0.7677INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 22s 112ms/step - loss: 0.7045 - sparse_categorical_accuracy: 0.7677 - val_loss: 0.9038 - val_sparse_categorical_accuracy: 0.6988\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 17s 85ms/step - loss: 0.6644 - sparse_categorical_accuracy: 0.7811 - val_loss: 0.9476 - val_sparse_categorical_accuracy: 0.6825\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 0.6321 - sparse_categorical_accuracy: 0.7959 - val_loss: 1.1363 - val_sparse_categorical_accuracy: 0.6237\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 22s 108ms/step - loss: 0.5920 - sparse_categorical_accuracy: 0.8080 - val_loss: 0.9283 - val_sparse_categorical_accuracy: 0.6888\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 18s 88ms/step - loss: 0.5678 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.9203 - val_sparse_categorical_accuracy: 0.6963\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5318 - sparse_categorical_accuracy: 0.8264INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 23s 113ms/step - loss: 0.5318 - sparse_categorical_accuracy: 0.8264 - val_loss: 0.8670 - val_sparse_categorical_accuracy: 0.7013\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 17s 83ms/step - loss: 0.5189 - sparse_categorical_accuracy: 0.8356 - val_loss: 0.8766 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 0.4861 - sparse_categorical_accuracy: 0.8458 - val_loss: 0.9419 - val_sparse_categorical_accuracy: 0.7038\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.4489 - sparse_categorical_accuracy: 0.8644INFO:tensorflow:Assets written to: ./callback_test_chkp/cnn_chkp_best/assets\n",
      "200/200 [==============================] - 25s 127ms/step - loss: 0.4489 - sparse_categorical_accuracy: 0.8644 - val_loss: 0.8595 - val_sparse_categorical_accuracy: 0.6975\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 15, 30, 128)       1152      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 15, 30, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 15, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 28, 128)        147456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 28, 128)        512       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 7, 28, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 26, 128)        147456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 26, 128)        512       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 3, 26, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 298,632\n",
      "Trainable params: 297,864\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "25/25 - 0s - loss: 0.8897 - sparse_categorical_accuracy: 0.7125\n",
      "\n",
      "acc: 0.7124999761581421, size: 257940 Inference Latency 0.4577467441558838ms\n",
      "\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.8697 - sparse_categorical_accuracy: 0.3531INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 1.8697 - sparse_categorical_accuracy: 0.3531 - val_loss: 1.6397 - val_sparse_categorical_accuracy: 0.3900\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.4676 - sparse_categorical_accuracy: 0.4847INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 1.4676 - sparse_categorical_accuracy: 0.4847 - val_loss: 1.3176 - val_sparse_categorical_accuracy: 0.5425\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.2290 - sparse_categorical_accuracy: 0.5803INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 40s 198ms/step - loss: 1.2290 - sparse_categorical_accuracy: 0.5803 - val_loss: 1.2302 - val_sparse_categorical_accuracy: 0.5663\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1032 - sparse_categorical_accuracy: 0.6219INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 1.1032 - sparse_categorical_accuracy: 0.6219 - val_loss: 1.1204 - val_sparse_categorical_accuracy: 0.6225\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.0058 - sparse_categorical_accuracy: 0.6547INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 42s 211ms/step - loss: 1.0058 - sparse_categorical_accuracy: 0.6547 - val_loss: 1.0276 - val_sparse_categorical_accuracy: 0.6525\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.9505 - sparse_categorical_accuracy: 0.6714 - val_loss: 1.0803 - val_sparse_categorical_accuracy: 0.6400\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8937 - sparse_categorical_accuracy: 0.6998INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.8937 - sparse_categorical_accuracy: 0.6998 - val_loss: 0.9184 - val_sparse_categorical_accuracy: 0.7063\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.8503 - sparse_categorical_accuracy: 0.7164INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 41s 207ms/step - loss: 0.8503 - sparse_categorical_accuracy: 0.7164 - val_loss: 0.8848 - val_sparse_categorical_accuracy: 0.7150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "200/200 [==============================] - 41s 206ms/step - loss: 0.8088 - sparse_categorical_accuracy: 0.7234 - val_loss: 0.8974 - val_sparse_categorical_accuracy: 0.7013\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 41s 203ms/step - loss: 0.7748 - sparse_categorical_accuracy: 0.7334 - val_loss: 0.8929 - val_sparse_categorical_accuracy: 0.7250\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7504 - sparse_categorical_accuracy: 0.7439INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.7504 - sparse_categorical_accuracy: 0.7439 - val_loss: 0.8502 - val_sparse_categorical_accuracy: 0.7175\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.7030 - sparse_categorical_accuracy: 0.7673INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 42s 211ms/step - loss: 0.7030 - sparse_categorical_accuracy: 0.7673 - val_loss: 0.7934 - val_sparse_categorical_accuracy: 0.7450\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 41s 204ms/step - loss: 0.6765 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.8026 - val_sparse_categorical_accuracy: 0.7387\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 40s 200ms/step - loss: 0.6755 - sparse_categorical_accuracy: 0.7714 - val_loss: 0.7990 - val_sparse_categorical_accuracy: 0.7375\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 39s 195ms/step - loss: 0.6386 - sparse_categorical_accuracy: 0.7823 - val_loss: 0.7998 - val_sparse_categorical_accuracy: 0.7475\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6139 - sparse_categorical_accuracy: 0.7992INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.6139 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.7857 - val_sparse_categorical_accuracy: 0.7700\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5973 - sparse_categorical_accuracy: 0.7986INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 38s 188ms/step - loss: 0.5973 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.7620 - val_sparse_categorical_accuracy: 0.7475\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.5574 - sparse_categorical_accuracy: 0.8095 - val_loss: 0.7940 - val_sparse_categorical_accuracy: 0.7312\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.5488 - sparse_categorical_accuracy: 0.8164 - val_loss: 0.7685 - val_sparse_categorical_accuracy: 0.7412\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.5291 - sparse_categorical_accuracy: 0.8253INFO:tensorflow:Assets written to: ./callback_test_chkp/dscnn_chkp_best/assets\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.5291 - sparse_categorical_accuracy: 0.8253 - val_loss: 0.7535 - val_sparse_categorical_accuracy: 0.7513\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 15, 30, 256)       2304      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 15, 30, 256)       1024      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 15, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d (DepthwiseC (None, 13, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 13, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 13, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_1 (Depthwis (None, 11, 26, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 26, 256)       65536     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 11, 26, 256)       1024      \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 11, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 2056      \n",
      "=================================================================\n",
      "Total params: 143,112\n",
      "Trainable params: 141,576\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "25/25 - 1s - loss: 0.7631 - sparse_categorical_accuracy: 0.7387\n",
      "\n",
      "acc: 0.7387499809265137, size: 293771 Inference Latency 0.8531653881072998ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "mfccs = True\n",
    "\n",
    "if not os.path.exists('data/mini_speech_commands'):\n",
    "    zip_path = tf.keras.utils.get_file(\n",
    "        origin='http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip',\n",
    "        fname='mini_speech_commands.zip',\n",
    "        extract=True,\n",
    "        cache_dir='.', cache_subdir='data')\n",
    "\n",
    "data_dir = os.path.join('.','data', 'mini_speech_commands')\n",
    "#filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "#filenames = tf.random.shuffle(filenames)\n",
    "#n = len(filenames)\n",
    "\n",
    "\n",
    "\n",
    "LABELS = np.array(tf.io.gfile.listdir(str(data_dir))) \n",
    "LABELS = [label for label in LABELS if label != 'README.md']\n",
    "\n",
    "class SignalGenerator:\n",
    "    def __init__(self, labels, sampling_rate=16000, frame_length=1920 , frame_step=960, num_mel_bins=40,\n",
    "                 lower_freq=20, upper_freq=48000, num_coefficients=10, mfccs=False):\n",
    "        self.frame_length = frame_length\n",
    "        self.frame_step = frame_step\n",
    "        self.mel_inputs =  [num_mel_bins, None, sampling_rate, lower_freq, upper_freq]\n",
    "        self.mfccs_coeff = num_coefficients\n",
    "        self.labels=labels\n",
    "        self.sampling_rate=sampling_rate\n",
    "        num_spectrogram_bins = (frame_length) // 2 + 1\n",
    "\n",
    "        if mfccs:\n",
    "            self.l2mel_matrix = linear_to_mel_weight_matrix(\n",
    "                    self.num_mel_bins, num_spectrogram_bins, self.sampling_rate,\n",
    "                    lower_freq, upper_freq)\n",
    "            self.preprocess = self.preprocess_with_mfcc\n",
    "        else:\n",
    "            self.preprocess = self.preprocess_with_stft\n",
    "\n",
    "    def read(self, file_path):\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\n",
    "        label = parts[-2]\n",
    "        label_id = tf.argmax(label == self.labels)\n",
    "        audio_binary = tf.io.read_file(file_path)\n",
    "        audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "        audio = tf.squeeze(audio, axis=1)\n",
    "        return audio, label_id\n",
    "\n",
    "    def pad(self, audio):\n",
    "        zero_padding = tf.zeros([self.sampling_rate] - tf.shape(audio), dtype=tf.float32)\n",
    "        audio = tf.concat([audio,zero_padding],0)\n",
    "        audio.set_shape([self.sampling_rate])\n",
    "        return audio\n",
    "\n",
    "    def get_spectrogram(self, audio):\n",
    "        tfstft = stft(audio, frame_length=self.frame_length, frame_step=self.frame_step,fft_length=self.frame_length)\n",
    "        spectrogram = tf.abs(tfstft)\n",
    "        return spectrogram\n",
    "\n",
    "    def get_mfcc(self, spectrogram):\n",
    "        mel_spectrogram = tensordot(spectrogram, self.l2mel_matrix, 1)\n",
    "        log_mel_spectrogram = log(mel_spectrogram + 1e-6)\n",
    "        mfccs = mfccs_from_log_mel_spectrograms(log_mel_spectrogram)[..., :self.mfccs_coeff]\n",
    "        return mfccs\n",
    "\n",
    "    def preprocess_with_stft(self, file_path):\n",
    "        audio, label = self.read(file_path)\n",
    "        audio = self.pad(audio)\n",
    "        spectrogram = self.get_spectrogram(audio)\n",
    "        spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "        spectrogram = tf.image.resize(spectrogram, [32,32])\n",
    "        return spectrogram, label\n",
    "\n",
    "    def preprocess_with_mfcc(self, file_path):\n",
    "        audio, label = self.read(file_path)\n",
    "        audio = self.pad(audio)\n",
    "        spectrogram = self.get_spectrogram(audio)\n",
    "        spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "        mfccs = get_mfcc(spectrogram)\n",
    "        return mfccs, label\n",
    "\n",
    "    def make_dataset(self, files, train):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "        ds = ds.map(self.preprocess, num_parallel_calls=4)\n",
    "        ds = ds.batch(32)\n",
    "        ds = ds.cache()\n",
    "\n",
    "        if train:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "        return ds\n",
    "\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.mkdir(dataset_dir)\n",
    "    train_files = tf.strings.split(tf.io.read_file('./kws_train_split.txt'),sep='\\n')[:-1]\n",
    "    val_files = tf.strings.split(tf.io.read_file('./kws_val_split.txt'),sep='\\n')[:-1]\n",
    "    test_files = tf.strings.split(tf.io.read_file('./kws_test_split.txt'),sep='\\n')[:-1]\n",
    "    generator = SignalGenerator(LABELS)\n",
    "    train_ds = generator.make_dataset(train_files, True)\n",
    "    val_ds = generator.make_dataset(val_files, False)\n",
    "    test_ds = generator.make_dataset(test_files, False)\n",
    "    tf.data.experimental.save(train_ds, f'{dataset_dir}/th_train')\n",
    "    tf.data.experimental.save(val_ds, f'{dataset_dir}/th_val')\n",
    "    tf.data.experimental.save(test_ds, f'{dataset_dir}/th_test')\n",
    "    \n",
    "\n",
    "stride = [2,2] if not mfccs else [2,1]\n",
    "\n",
    "MLP = keras.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=256,activation=keras.activations.relu),\n",
    "    keras.layers.Dense(units=256,activation=keras.activations.relu),\n",
    "    keras.layers.Dense(units=256,activation=keras.activations.relu),\n",
    "    keras.layers.Dense(units=len(LABELS))\n",
    "])\n",
    "\n",
    "CNN = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=[3,3], strides = stride, use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.1),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=[3,3], strides = stride, use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.1),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=[3,3], strides = stride, use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.1),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(units=len(LABELS))\n",
    "])\n",
    "\n",
    "DSCNN = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=stride, use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.1),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=[1, 1], strides=[1, 1], use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.1),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=[1, 1], strides=[1, 1], use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.1),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(units=len(LABELS))\n",
    "])\n",
    "\n",
    "models = [MLP,CNN,DSCNN]\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    model.compile(optimizer='adam',loss=loss, metrics=[metric])\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        f'./callback_test_chkp/{mods_names[i]}_chkp_best',\n",
    "        monitor='val_loss',\n",
    "        verbose=0, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='auto',\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "    model.fit(train_ds, batch_size=32, epochs=20, validation_data=val_ds,callbacks=[cp_callback])\n",
    "    model.summary()\n",
    "    start = t.time()\n",
    "    test_loss, test_acc2 = model.evaluate(test_ds, verbose=2)\n",
    "    end = t.time() - start\n",
    "    msize = os.path.getsize(f'./callback_test_chkp/{mods_names[i]}_chkp_best/saved_model.pb')\n",
    "    print()\n",
    "    print(f'acc: {test_acc2}, size: {msize} Inference Latency {end}ms')\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saves and run tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of basic model: 555.7421875kB\n",
      "size of optimized model: 146.3125kB \n",
      "compressed: 114.5927734375kB\n",
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.float32'>\n",
      "accuracy: 0.70375 tflite size: 146.3125kB compressed: 114.5927734375kB time: 7.691739320755005ms\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.lite as tflite\n",
    "import zlib\n",
    "#import tensorflow_model_optimization as tfmot\n",
    "#pruning_params = {'pruning_schedule':tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.30, \n",
    "#                                                                          final_sparsity=0.8,\n",
    "#                                                                          begin_step=len(train_ds)*5,\n",
    "#                                                                          end_step=len(train_ds)*15)}\n",
    "#prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "\n",
    "tensor_specs = (tf.TensorSpec([None,32,32,1],dtype=tf.float32),tf.TensorSpec([None,],dtype=tf.int64))\n",
    "train_ds = tf.data.experimental.load(f'{dataset_dir}/th_train',tensor_specs)\n",
    "val_ds = tf.data.experimental.load(f'{dataset_dir}/th_val', tensor_specs)\n",
    "test_ds = tf.data.experimental.load(f'{dataset_dir}/th_test',tensor_specs)\n",
    "test_ds = test_ds.unbatch().batch(1)\n",
    "\n",
    "def convert(img, target_type_min, target_type_max, target_type):\n",
    "    img = img\n",
    "    imin = img.min()\n",
    "    imax = img.max()\n",
    "\n",
    "    a = (target_type_max - target_type_min) / (imax - imin)\n",
    "    b = target_type_max - a * imax\n",
    "    new_img = (a * img + b).astype(target_type)\n",
    "    return new_img\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for x, _ in train_ds.take(100):\n",
    "        yield [x]\n",
    "    \n",
    "for mod in mods_names:\n",
    "    #saving\n",
    "    tflite_dirs = './tflite_models'\n",
    "    \n",
    "    if mod != 'dscnn':\n",
    "        continue\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(f'./callback_test_chkp/{mod}_chkp_best/')\n",
    "    tflite_model = converter.convert()\n",
    "    if not os.path.exists(tflite_dirs): \n",
    "        os.mkdir(tflite_dirs)\n",
    "    with open(tflite_dirs+f'/{mod}_basic.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    tflo_size=os.path.getsize(tflite_dirs+f\"/{mod}_basic.tflite\")\n",
    "    print(f'size of basic model: {tflo_size/1024}kB')\n",
    "    \n",
    "    '''\n",
    "    tflite_opt_model = converter.convert()\n",
    "    if not os.path.exists(tflite_dirs): \n",
    "        os.mkdir(tflite_dirs)\n",
    "    with open(tflite_dirs+f'/{mod}_opt.tflite', 'wb') as f:\n",
    "        f.write(tflite_opt_model)\n",
    "    tflo_size=os.path.getsize(tflite_dirs+f\"/{mod}_opt.tflite\")\n",
    "    print(f'size of optimized model: {tflo_size/1024}kB')\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(tflite_dirs+f'/{mod}_opt.tflite')\n",
    "    '''\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(f'./callback_test_chkp/{mod}_chkp_best/')\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    #converter.representative_dataset = representative_dataset_gen\n",
    "    #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    #converter.inference_input_type = tf.uint8  # or tf.uint8\n",
    "    #converter.inference_output_type = tf.uint8 \n",
    "    tflite_quant_model = converter.convert()\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(tflite_dirs): \n",
    "        os.mkdir(tflite_dirs)\n",
    "    with open(tflite_dirs+f'/{mod}.tflite', 'wb') as f:\n",
    "        f.write(tflite_quant_model)\n",
    "    tfl_size=os.path.getsize(tflite_dirs+f\"/{mod}.tflite\")\n",
    "    with open(tflite_dirs+f\"/{mod}_compressed.tflite.zlib\", 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_quant_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    tflc_size=os.path.getsize(tflite_dirs+f\"/{mod}_compressed.tflite.zlib\")\n",
    "    \n",
    "    print(f'size of optimized model: {tfl_size/1024}kB \\ncompressed: {tflc_size/1024}kB')\n",
    "    \n",
    "    interpreter = tflite.Interpreter(model_path = tflite_dirs+f\"/{mod}.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    print('input: ', input_details[0]['dtype'])\n",
    "    output_details = interpreter.get_output_details()\n",
    "    print('output: ', output_details[0]['dtype'])\n",
    "\n",
    "    input_shape = input_details[0]['shape']\n",
    "    num_corr = 0\n",
    "    num = 0\n",
    "    start = t.time()\n",
    "    for input_data,label in test_ds:\n",
    "        #input_data = convert(input_data, 0, 255, np.uint8)\n",
    "        #label = convert(label, 0, 255, np.uint8)\n",
    "        #input_data = tf.quantization.quantize(input_data,min(input_data),max(input_data),tf.quint8)\n",
    "        #label = tf.quantization.quantize(label,min(LABELS),max(LABELS),tf.quint8)\n",
    "        num += 1\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        output_data = np.argmax(interpreter.get_tensor(output_details[0]['index']))\n",
    "        y_pred = tf.constant([output_data],dtype=tf.int64)\n",
    "        if label.numpy()[0] == output_data:\n",
    "            num_corr+=1\n",
    "    end = t.time() - start\n",
    "    print(f'accuracy: {num_corr/num} tflite size: {tfl_size/1024}kB compressed: {tflc_size/1024}kB time: {end}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = prune_low_magnitude(model, **pruning_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
